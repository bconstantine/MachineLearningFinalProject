{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bryan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Bryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Bryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree, naive_bayes\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import graphviz\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, balanced_accuracy_score\n",
    "import optuna\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# acllmdb Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b, c, random_state=42):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "    assert len(a) == len(b)\n",
    "    assert len(a) == len(c)\n",
    "    p = np.random.RandomState(seed=random_state).permutation(len(a))\n",
    "    return a[p], b[p], c[p]\n",
    "\n",
    "def read_contents_update_array(Dirname, filename, textArray, decimalLabelArray, integerLabelArray):\n",
    "    originalLabel = filename.split(\"_\")[1].split(\".\")[0]\n",
    "    normalizedLabel = (int(originalLabel) - 1) / 9\n",
    "    decimalLabelArray.append(normalizedLabel)\n",
    "    integerLabelArray.append(round(normalizedLabel))\n",
    "    \n",
    "    with open(Dirname + filename, 'r', encoding='latin-1') as f:\n",
    "        textArray.append(f.read())\n",
    "\n",
    "    return textArray, decimalLabelArray, integerLabelArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reach here\n"
     ]
    }
   ],
   "source": [
    "# group the supervised into train and test csv\n",
    "# rating is 1 - 10\n",
    "# for better scale, we normalize the data to between 0 - 1\n",
    "textArray = []\n",
    "decimalLabelArray = []\n",
    "integerLabelArray = []\n",
    "motherDir = \"./aclImdb/\"\n",
    "for filename in os.listdir(motherDir + \"train/pos\"):\n",
    "    textArray, decimalLabelArray, integerLabelArray = read_contents_update_array(motherDir + \"train/pos/\", filename, textArray, decimalLabelArray, integerLabelArray)\n",
    "print(\"Reach here\")\n",
    "for filename in os.listdir(motherDir + \"train/neg\"):\n",
    "    textArray, decimalLabelArray, integerLabelArray = read_contents_update_array(motherDir + \"train/neg/\", filename, textArray, decimalLabelArray, integerLabelArray)\n",
    "\n",
    "textArray, decimalLabelArray, integerLabelArray = unison_shuffled_copies(textArray, decimalLabelArray, integerLabelArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'text': textArray, 'decimalLabel': decimalLabelArray, 'integerLabel': integerLabelArray})\n",
    "train_df.to_csv(\"aclimdb/train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reach here\n"
     ]
    }
   ],
   "source": [
    "# group the supervised into train and test csv\n",
    "# rating is 1 - 10\n",
    "# for better scale, we normalize the data to between 0 - 1\n",
    "textArray = []\n",
    "decimalLabelArray = []\n",
    "integerLabelArray = []\n",
    "motherDir = \"./aclImdb/\"\n",
    "for filename in os.listdir(motherDir + \"test/pos\"):\n",
    "    textArray, decimalLabelArray, integerLabelArray = read_contents_update_array(motherDir + \"test/pos/\", filename, textArray, decimalLabelArray, integerLabelArray)\n",
    "print(\"Reach here\")\n",
    "for filename in os.listdir(motherDir + \"test/neg\"):\n",
    "    textArray, decimalLabelArray, integerLabelArray = read_contents_update_array(motherDir + \"test/neg/\", filename, textArray, decimalLabelArray, integerLabelArray)\n",
    "\n",
    "textArray, decimalLabelArray, integerLabelArray = unison_shuffled_copies(textArray, decimalLabelArray, integerLabelArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'text': textArray, 'decimalLabel': decimalLabelArray, 'integerLabel': integerLabelArray})\n",
    "test_df.to_csv(\"aclimdb/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV, create heuristic characteristic for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acl = pd.read_csv(\"aclimdb/train.csv\")\n",
    "test_acl = pd.read_csv(\"aclimdb/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning noun extraction\n"
     ]
    }
   ],
   "source": [
    "#Try to extract several variables from text\n",
    "#simple features\n",
    "def splitSentenceInPredefinedSeparator(x):\n",
    "    separators = [',', ' ', '.']\n",
    "    for separator in separators:\n",
    "        x = x.replace(separator, ' ')\n",
    "    return str(x).split()\n",
    "\n",
    "#Count number of words\n",
    "def numberOfWords(x):\n",
    "    #return number of words that has been splitted including with ',', '.', ' '\n",
    "    return len(splitSentenceInPredefinedSeparator(x))\n",
    "\n",
    "def uniqueWords(x):\n",
    "    #return number of unique words that has been splitted including with ',', '.', ' '\n",
    "    return len(set(splitSentenceInPredefinedSeparator(x)))\n",
    "\n",
    "\n",
    "train_acl['num-words'] = train_acl['text'].apply(lambda x: numberOfWords(x))\n",
    "test_acl['num-words'] = test_acl['text'].apply(lambda x: numberOfWords(x))\n",
    "#Count number of unique words\n",
    "train_acl['num-unique-words'] = train_acl['text'].apply(lambda x: uniqueWords(x))\n",
    "test_acl['num-unique-words'] = test_acl['text'].apply(lambda x: uniqueWords(x))\n",
    "#Count number of punctuations\n",
    "train_acl['num-punctuations'] = train_acl['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "test_acl['num-punctuations'] = test_acl['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "#UniqueWordRatio\n",
    "train_acl['unique-word-ratio'] = train_acl['num-unique-words'] / train_acl['num-words']\n",
    "test_acl['unique-word-ratio'] = test_acl['num-unique-words'] / test_acl['num-words']\n",
    "#PunctuationRatio\n",
    "train_acl['punctuation-ratio'] = train_acl['num-punctuations'] / train_acl['num-words']\n",
    "test_acl['punctuation-ratio'] = test_acl['num-punctuations'] / test_acl['num-words']\n",
    "#Sentence Length\n",
    "train_acl['sentence-length'] = train_acl['text'].apply(lambda x: len(x.split('.')))\n",
    "test_acl['sentence-length'] = test_acl['text'].apply(lambda x: len(x.split('.')))\n",
    "#Number of Words beginning with capital letters (Other than first word in a sentence after period)\n",
    "train_acl['num-words-beginning-capital'] = train_acl['text'].apply(lambda x: len([w for w in x.split() if w[0].isupper() and w[0] not in ['.', '\"', \"'\"]]))\n",
    "test_acl['num-words-beginning-capital'] = test_acl['text'].apply(lambda x: len([w for w in x.split() if w[0].isupper() and w[0] not in ['.', '\"', \"'\"]]))\n",
    "#Count Number of Nouns, and Nouns Ratio\n",
    "print(\"beginning noun extraction\")\n",
    "train_acl['num-nouns'] = train_acl['text'].apply(lambda x: len([w for w in word_tokenize(x) if pos_tag([w])[0][1] in ['NN', 'NNP']]))\n",
    "test_acl['num-nouns'] = test_acl['text'].apply(lambda x: len([w for w in word_tokenize(x) if pos_tag([w])[0][1] in ['NN', 'NNP']]))\n",
    "train_acl['nouns-ratio'] = train_acl['num-nouns'] / train_acl['num-words']\n",
    "test_acl['nouns-ratio'] = test_acl['num-nouns'] / test_acl['num-words']\n",
    "#Count Number of Verbs, and Verbs Ratio\n",
    "print(\"beginning verb extraction\")\n",
    "train_acl['num-verbs'] = train_acl['text'].apply(lambda x: len([w for w in word_tokenize(x) if pos_tag([w])[0][1] in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']]))\n",
    "test_acl['num-verbs'] = test_acl['text'].apply(lambda x: len([w for w in word_tokenize(x) if pos_tag([w])[0][1] in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']]))\n",
    "train_acl['verbs-ratio'] = train_acl['num-verbs'] / train_acl['num-words']\n",
    "test_acl['verbs-ratio'] = test_acl['num-verbs'] / test_acl['num-words']\n",
    "#Count Number of Adjectives, and Adjectives Ratio\n",
    "print(\"beginning adjective extraction\")\n",
    "train_acl['num-adjectives'] = train_acl['text'].apply(lambda x: len([w for w in word_tokenize(x) if pos_tag([w])[0][1] in ['JJ', 'JJR', 'JJS']]))\n",
    "test_acl['num-adjectives'] = test_acl['text'].apply(lambda x: len([w for w in word_tokenize(x) if pos_tag([w])[0][1] in ['JJ', 'JJR', 'JJS']]))\n",
    "train_acl['adjectives-ratio'] = train_acl['num-adjectives'] / train_acl['num-words']\n",
    "test_acl['adjectives-ratio'] = test_acl['num-adjectives'] / test_acl['num-words']\n",
    "#Average Word Length\n",
    "train_acl['average-word-length'] = train_acl['text'].apply(lambda x: np.mean([len(w) for w in splitSentenceInPredefinedSeparator(x)]))\n",
    "test_acl['average-word-length'] = test_acl['text'].apply(lambda x: np.mean([len(w) for w in splitSentenceInPredefinedSeparator(x)]))\n",
    "#stopwords count\n",
    "train_acl['stopwords-count'] = train_acl['text'].apply(lambda x: len([w for w in splitSentenceInPredefinedSeparator(x) if w.lower() in nltk.corpus.stopwords.words('english')]))\n",
    "test_acl['stopwords-count'] = test_acl['text'].apply(lambda x: len([w for w in splitSentenceInPredefinedSeparator(x) if w.lower() in nltk.corpus.stopwords.words('english')]))\n",
    "train_acl['Stopwords-ratio'] = train_acl['stopwords-count'] / test_acl['num-words']\n",
    "test_acl['Stopwords-ratio'] = test_acl['stopwords-count'] / test_acl['num-words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acl.to_csv(\"aclimdb/train_heuristic.csv\", index=False)\n",
    "test_acl.to_csv(\"aclimdb/test_heuristic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Labelled Sentences Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
