{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bryan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Bryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Bryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree, naive_bayes\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import graphviz\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, balanced_accuracy_score\n",
    "import optuna\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acl = pd.read_csv(\"aclimdb/train.csv\")\n",
    "test_acl = pd.read_csv(\"aclimdb/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to extract several variables from text\n",
    "#simple features\n",
    "def splitSentenceInPredefinedSeparator(x):\n",
    "    separators = [',', ' ', '.']\n",
    "    for separator in separators:\n",
    "        x = x.replace(separator, ' ')\n",
    "    return str(x).split()\n",
    "\n",
    "#Count number of words\n",
    "def numberOfWords(x):\n",
    "    #return number of words that has been splitted including with ',', '.', ' '\n",
    "    return len(splitSentenceInPredefinedSeparator(x))\n",
    "\n",
    "def uniqueWords(x):\n",
    "    #return number of unique words that has been splitted including with ',', '.', ' '\n",
    "    return len(set(splitSentenceInPredefinedSeparator(x)))\n",
    "\n",
    "\n",
    "train_acl['num-words'] = train_acl['text'].apply(lambda x: numberOfWords(x))\n",
    "test_acl['num-words'] = test_acl['text'].apply(lambda x: numberOfWords(x))\n",
    "#Count number of unique words\n",
    "train_acl['num-unique-words'] = train_acl['text'].apply(lambda x: uniqueWords(x))\n",
    "test_acl['num-unique-words'] = test_acl['text'].apply(lambda x: uniqueWords(x))\n",
    "#Count number of punctuations\n",
    "train_acl['num-punctuations'] = train_acl['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "test_acl['num-punctuations'] = test_acl['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "#UniqueWordRatio\n",
    "train_acl['unique-word-ratio'] = train_acl['num-unique-words'] / train_acl['num-words']\n",
    "test_acl['unique-word-ratio'] = test_acl['num-unique-words'] / test_acl['num-words']\n",
    "#PunctuationRatio\n",
    "train_acl['punctuation-ratio'] = train_acl['num-punctuations'] / train_acl['num-words']\n",
    "test_acl['punctuation-ratio'] = test_acl['num-punctuations'] / test_acl['num-words']\n",
    "#Sentence Length\n",
    "train_acl['sentence-length'] = train_acl['text'].apply(lambda x: len(x.split('.')))\n",
    "test_acl['sentence-length'] = test_acl['text'].apply(lambda x: len(x.split('.')))\n",
    "#Number of Words beginning with capital letters (Other than first word in a sentence after period)\n",
    "train_acl['num-words-beginning-capital'] = train_acl['text'].apply(lambda x: len([w for w in x.split() if w[0].isupper() and w[0] not in ['.', '\"', \"'\"]]))\n",
    "test_acl['num-words-beginning-capital'] = test_acl['text'].apply(lambda x: len([w for w in x.split() if w[0].isupper() and w[0] not in ['.', '\"', \"'\"]]))\n",
    "# #Count Number of Nouns, and Nouns Ratio\n",
    "# print(\"beginning noun extraction\")\n",
    "# train_acl['num-nouns'] = train_acl['text'].apply(lambda x: len([w for w in word_tokenize(x) if pos_tag([w])[0][1] in ['NN', 'NNP']]))\n",
    "# test_acl['num-nouns'] = test_acl['text'].apply(lambda x: len([w for w in word_tokenize(x) if pos_tag([w])[0][1] in ['NN', 'NNP']]))\n",
    "# train_acl['nouns-ratio'] = train_acl['num-nouns'] / train_acl['num-words']\n",
    "# test_acl['nouns-ratio'] = test_acl['num-nouns'] / test_acl['num-words']\n",
    "# #Count Number of Verbs, and Verbs Ratio\n",
    "# print(\"beginning verb extraction\")\n",
    "# train_acl['num-verbs'] = train_acl['text'].apply(lambda x: len([w for w in word_tokenize(x) if pos_tag([w])[0][1] in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']]))\n",
    "# test_acl['num-verbs'] = test_acl['text'].apply(lambda x: len([w for w in word_tokenize(x) if pos_tag([w])[0][1] in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']]))\n",
    "# train_acl['verbs-ratio'] = train_acl['num-verbs'] / train_acl['num-words']\n",
    "# test_acl['verbs-ratio'] = test_acl['num-verbs'] / test_acl['num-words']\n",
    "# #Count Number of Adjectives, and Adjectives Ratio\n",
    "# print(\"beginning adjective extraction\")\n",
    "# train_acl['num-adjectives'] = train_acl['text'].apply(lambda x: len([w for w in word_tokenize(x) if pos_tag([w])[0][1] in ['JJ', 'JJR', 'JJS']]))\n",
    "# test_acl['num-adjectives'] = test_acl['text'].apply(lambda x: len([w for w in word_tokenize(x) if pos_tag([w])[0][1] in ['JJ', 'JJR', 'JJS']]))\n",
    "# train_acl['adjectives-ratio'] = train_acl['num-adjectives'] / train_acl['num-words']\n",
    "# test_acl['adjectives-ratio'] = test_acl['num-adjectives'] / test_acl['num-words']\n",
    "# #Average Word Length\n",
    "# train_acl['average-word-length'] = train_acl['text'].apply(lambda x: np.mean([len(w) for w in splitSentenceInPredefinedSeparator(x)]))\n",
    "# test_acl['average-word-length'] = test_acl['text'].apply(lambda x: np.mean([len(w) for w in splitSentenceInPredefinedSeparator(x)]))\n",
    "# #stopwords count\n",
    "# train_acl['stopwords-count'] = train_acl['text'].apply(lambda x: len([w for w in splitSentenceInPredefinedSeparator(x) if w.lower() in nltk.corpus.stopwords.words('english')]))\n",
    "# test_acl['stopwords-count'] = test_acl['text'].apply(lambda x: len([w for w in splitSentenceInPredefinedSeparator(x) if w.lower() in nltk.corpus.stopwords.words('english')]))\n",
    "# train_acl['Stopwords-ratio'] = train_acl['stopwords-count'] / test_acl['num-words']\n",
    "# test_acl['Stopwords-ratio'] = test_acl['stopwords-count'] / test_acl['num-words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning Adjective extraction\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n"
     ]
    }
   ],
   "source": [
    "#Adjective extraction\n",
    "print(\"beginning Adjective extraction\")\n",
    "train_num_adj = []\n",
    "test_num_adj = []\n",
    "for i in range(len(train_acl)):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    train_num_adj.append(len([w for w in word_tokenize(train_acl['text'][i]) if pos_tag([w])[0][1] in ['JJ', 'JJR', 'JJS']]))\n",
    "    test_num_adj.append(len([w for w in word_tokenize(test_acl['text'][i]) if pos_tag([w])[0][1] in ['JJ', 'JJR', 'JJS']]))\n",
    "train_acl['num-adjs'] = train_num_adj\n",
    "test_acl['num-adjs'] = test_num_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acl['adjs-ratio'] = train_acl['num-adjs'] / train_acl['num-words']\n",
    "test_acl['adjs-ratio'] = test_acl['num-adjs'] / test_acl['num-words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acl.to_csv(\"aclimdb/train_heuristic_adjs.csv\", index=False)\n",
    "test_acl.to_csv(\"aclimdb/test_heuristic_adjs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
